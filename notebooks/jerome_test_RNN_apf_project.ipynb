{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qAlSZB7RAvX"
      },
      "source": [
        "# Imports and Define Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OXeNNT3CEtjO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-10 21:55:50.293946: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-03-10 21:55:52.003051: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-03-10 21:55:52.009438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-03-10 21:55:59.510273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from colorama import Fore, Style\n",
        "from tensorflow import keras\n",
        "from google.cloud import storage\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from colorama import Fore, Style\n",
        "from dateutil.parser import parse\n",
        "from typing import Dict, List, Tuple, Sequence\n",
        "from datetime import datetime\n",
        "\n",
        "from power.params import *\n",
        "from power.ml_ops.data import get_data_with_cache, load_data_to_bq, clean_pv_data\n",
        "from power.ml_ops.model import initialize_model, compile_model, train_model\n",
        "from power.ml_ops.registry import load_model, save_model, save_results\n",
        "from power.ml_ops.cross_val import get_Xi_yi, get_X_y_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa11TRvzRF9A"
      },
      "source": [
        "# Package Functions as reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AfGdALOiFljf"
      },
      "outputs": [],
      "source": [
        "def preprocess(start_date:str = '1980-01-01',\n",
        "               stop_date:str = '2022-12-31') -> None:\n",
        "    \"\"\"\n",
        "    - Query the raw dataset from Le Wagon's BigQuery dataset\n",
        "    - Cache query result as a local CSV if it doesn't exist locally\n",
        "    - Process query data\n",
        "    - Store processed data on your personal BQ (truncate existing table if it exists)\n",
        "    - No need to cache processed data as CSV (it will be cached when queried back from BQ during training)\n",
        "    \"\"\"\n",
        "\n",
        "    print(Fore.MAGENTA + \"\\n ‚≠êÔ∏è Use case: preprocess\" + Style.RESET_ALL)\n",
        "\n",
        "    # Query raw data from BUCKET BigQuery using `get_data_with_cache`\n",
        "    query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM {GCP_PROJECT}.{BQ_DATASET}.raw_pv\n",
        "        ORDER BY _0\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve data using `get_data_with_cache`\n",
        "    data_query_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"raw\", f\"raw_pv.csv\")\n",
        "    data_query = get_data_with_cache(\n",
        "        query=query,\n",
        "        gcp_project=GCP_PROJECT,\n",
        "        cache_path=data_query_cache_path,\n",
        "        data_has_header=True\n",
        "    )\n",
        "\n",
        "    # Process data\n",
        "    data_clean = clean_pv_data(data_query)\n",
        "\n",
        "\n",
        "    load_data_to_bq(\n",
        "        data_clean,\n",
        "        gcp_project=GCP_PROJECT,\n",
        "        bq_dataset=BQ_DATASET,\n",
        "        table=f'processed_pv',\n",
        "        truncate=True\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ preprocess() done \\n\")\n",
        "\n",
        "\n",
        "\n",
        "def train(\n",
        "        start_date:str = '1980-01-01',\n",
        "        stop_date:str = '2019-12-30',\n",
        "        split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set\n",
        "        learning_rate=0.02,\n",
        "        batch_size = 32,\n",
        "        patience = 5\n",
        "    ) -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    - Download processed data from your BQ table (or from cache if it exists)\n",
        "    - Train on the preprocessed dataset (which should be ordered by date)\n",
        "    - Store training results and model weights\n",
        "\n",
        "    Return val_mae as a float\n",
        "    \"\"\"\n",
        "\n",
        "    print(Fore.MAGENTA + \"\\n‚≠êÔ∏è Use case: train\" + Style.RESET_ALL)\n",
        "    print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
        "\n",
        "\n",
        "    # Load processed data using `get_data_with_cache` in chronological order\n",
        "    query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
        "        ORDER BY utc_time\n",
        "    \"\"\"\n",
        "\n",
        "    data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
        "    data_processed = get_data_with_cache(\n",
        "        gcp_project=GCP_PROJECT,\n",
        "        query=query,\n",
        "        cache_path=data_processed_cache_path,\n",
        "        data_has_header=True\n",
        "    )\n",
        "\n",
        "    # the model uses power as feature -> fix that in raw data\n",
        "    data_processed = data_processed.rename(columns={'electricity': 'power'})\n",
        "    # the processed data from bq needs to be converted to datetime object\n",
        "    data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)\n",
        "\n",
        "    if data_processed.shape[0] < 240:\n",
        "        print(\"‚ùå Not enough processed data retrieved to train on\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train = data_processed[data_processed['utc_time'] < '2020-01-01']\n",
        "    test = data_processed[data_processed['utc_time'] >= '2020-01-01']\n",
        "\n",
        "    train = train[['power']]\n",
        "    test = test[['power']]\n",
        "\n",
        "    X_train, y_train = get_X_y_seq(train,\n",
        "                                   number_of_sequences=10_000,\n",
        "                                   input_length=48,\n",
        "                                   output_length=24)\n",
        "\n",
        "\n",
        "    # Train model using `model.py`\n",
        "    model = load_model()\n",
        "\n",
        "    if model is None:\n",
        "        model = initialize_model(X_train, y_train, n_unit=24)\n",
        "\n",
        "    model = compile_model(model, learning_rate=learning_rate)\n",
        "    model, history = train_model(model,\n",
        "                                X_train,\n",
        "                                y_train,\n",
        "                                validation_split = 0.3,\n",
        "                                batch_size = 32,\n",
        "                                epochs = 50\n",
        "                                )\n",
        "\n",
        "    val_mae = np.min(history.history['val_mae'])\n",
        "\n",
        "    params = dict(\n",
        "        context=\"train\",\n",
        "        training_set_size='40 years worth of data',\n",
        "        row_count=len(X_train),\n",
        "    )\n",
        "\n",
        "    # Save results on the hard drive using taxifare.ml_logic.registry\n",
        "    save_results(params=params, metrics=dict(mae=val_mae))\n",
        "\n",
        "    # Save model weight on the hard drive (and optionally on GCS too!)\n",
        "    save_model(model=model)\n",
        "\n",
        "    print(\"‚úÖ train() done \\n\")\n",
        "\n",
        "    return val_mae\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "        min_date:str = '2014-01-01',\n",
        "        max_date:str = '2015-01-01',\n",
        "        stage: str = \"Production\"\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate the performance of the latest production model on processed data\n",
        "    Return MAE as a float\n",
        "    \"\"\"\n",
        "    print(Fore.MAGENTA + \"\\n‚≠êÔ∏è Use case: evaluate\" + Style.RESET_ALL)\n",
        "\n",
        "    model = load_model(stage=stage)\n",
        "    assert model is not None\n",
        "\n",
        "\n",
        "    # Query your BigQuery processed table and get data_processed using `get_data_with_cache`\n",
        "    query = f\"\"\"\n",
        "        SELECT *\n",
        "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
        "        ORDER BY utc_time\n",
        "    \"\"\"\n",
        "\n",
        "    data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
        "    data_processed = get_data_with_cache(\n",
        "        gcp_project=GCP_PROJECT,\n",
        "        query=query,\n",
        "        cache_path=data_processed_cache_path,\n",
        "        data_has_header=True\n",
        "    )\n",
        "\n",
        "    if data_processed.shape[0] == 0:\n",
        "        print(\"‚ùå No data to evaluate on\")\n",
        "        return None\n",
        "\n",
        "    data_processed = data_processed.to_numpy()\n",
        "\n",
        "    X_new = data_processed[:, :-1]\n",
        "    y_new = data_processed[:, -1]\n",
        "\n",
        "    metrics_dict = evaluate_model(model=model, X=X_new, y=y_new)\n",
        "    mae = metrics_dict[\"mae\"]\n",
        "\n",
        "    params = dict(\n",
        "        context=\"evaluate\", # Package behavior\n",
        "        training_set_size=DATA_SIZE,\n",
        "        row_count=len(X_new)\n",
        "    )\n",
        "\n",
        "    save_results(params=params, metrics=metrics_dict)\n",
        "\n",
        "    print(\"‚úÖ evaluate() done \\n\")\n",
        "\n",
        "    return mae\n",
        "\n",
        "\n",
        "def pred(X_pred:str = '2013-05-08 12:00:00') -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Make a prediction using the latest trained model\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n‚≠êÔ∏è Use case: predict\")\n",
        "\n",
        "    # X_pred = datetime.strptime(X_pred, '%Y-%m-%d %H:%M:%S')\n",
        "    # reference_datetime = datetime.strptime(\"1980-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S')\n",
        "    # time_difference = X_pred - reference_datetime\n",
        "    # time_difference_hours = time_difference.total_seconds() / 3600\n",
        "    # input_date = X_test[time_difference_hours-47: time_difference_hours+1]\n",
        "\n",
        "\n",
        "\n",
        "    # if X_pred is None:\n",
        "    #     X_pred = pd.DataFrame(dict(\n",
        "    #     pickup_datetime=[pd.Timestamp(\"2013-07-06 17:18:00\", tz='UTC')],\n",
        "    #     pickup_longitude=[-73.950655],\n",
        "    #     pickup_latitude=[40.783282],\n",
        "    #     dropoff_longitude=[-73.984365],\n",
        "    #     dropoff_latitude=[40.769802],\n",
        "    #     passenger_count=[1],\n",
        "    # ))\n",
        "\n",
        "    model = load_model()\n",
        "    assert model is not None\n",
        "\n",
        "    # X_processed = preprocess_features(X_pred)\n",
        "    # y_pred = model.predict(X_processed)\n",
        "\n",
        "    # print(\"\\n‚úÖ prediction done: \", y_pred, y_pred.shape, \"\\n\")\n",
        "    print(\"\\n‚úÖ prediction done: \\n\")\n",
        "    return model # change it back! to return y_pred\n",
        "    # return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35m\n",
            " ‚≠êÔ∏è Use case: preprocess\u001b[0m\n",
            "\u001b[34m\n",
            "Load data from BigQuery server...\u001b[0m\n",
            "‚úÖ Data loaded, with shape (376944, 8)\n",
            "# data cleaned\n",
            "\u001b[34m\n",
            "Save data to BigQuery @ le-wagon-data-411310.power.processed_pv...:\u001b[0m\n",
            "\n",
            "Write le-wagon-data-411310.power.processed_pv (376944 rows)\n",
            "‚úÖ Data saved to bigquery, with shape (376944, 3)\n",
            "‚úÖ preprocess() done \n",
            "\n"
          ]
        }
      ],
      "source": [
        "preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35m\n",
            "‚≠êÔ∏è Use case: train\u001b[0m\n",
            "\u001b[34m\n",
            "Loading preprocessed validation data...\u001b[0m\n",
            "\u001b[34m\n",
            "Load data from BigQuery server...\u001b[0m\n",
            "‚úÖ Data loaded, with shape (376944, 3)\n",
            "\u001b[34m\n",
            "Load latest model from GCS...\u001b[0m\n",
            "\n",
            "‚ùå No model found in GCS bucket power_jerome-roeser\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-10 11:55:49.969873: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2024-03-10 11:55:49.975609: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2024-03-10 11:55:49.979680: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
            "2024-03-10 11:55:51.158016: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2024-03-10 11:55:51.164167: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2024-03-10 11:55:51.171898: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
            "2024-03-10 11:55:53.075756: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2024-03-10 11:55:53.079311: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2024-03-10 11:55:53.082233: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
            "2024-03-10 11:56:03.834513: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2024-03-10 11:56:03.838429: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2024-03-10 11:56:03.845886: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Results saved locally\n",
            "‚úÖ Model saved locally\n",
            "\u001b[34m\n",
            "Save model to GCS @ power_jerome-roeser...\u001b[0m\n",
            "‚úÖ Model saved to GCS\n",
            "‚úÖ train() done \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.05749411880970001"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\n",
            "Load latest model from GCS...\u001b[0m\n",
            "‚úÖ Latest model downloaded from cloud storage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-10 12:05:16.121002: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
            "2024-03-10 12:05:16.126086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
            "2024-03-10 12:05:16.129462: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
            "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
          ]
        }
      ],
      "source": [
        "model = load_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTJNnbpCROmz"
      },
      "source": [
        "# Load Processed data and split into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5CsTRK4GzQv",
        "outputId": "5b94b49f-2d5e-4a03-bb86-4f09face50fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35m\n",
            "‚≠êÔ∏è Use case: train\u001b[0m\n",
            "\u001b[34m\n",
            "Loading preprocessed validation data...\u001b[0m\n",
            "\u001b[34m\n",
            "Load data from local CSV...\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data loaded, with shape (376944, 3)\n"
          ]
        }
      ],
      "source": [
        "print(Fore.MAGENTA + \"\\n‚≠êÔ∏è Use case: train\" + Style.RESET_ALL)\n",
        "print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
        "\n",
        "\n",
        "# Load processed data using `get_data_with_cache` in chronological order\n",
        "\n",
        "query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
        "    ORDER BY utc_time\n",
        "\"\"\"\n",
        "\n",
        "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
        "data_processed = get_data_with_cache(\n",
        "    gcp_project=GCP_PROJECT,\n",
        "    query=query,\n",
        "    cache_path=data_processed_cache_path,\n",
        "    data_has_header=True\n",
        ")\n",
        "\n",
        "# the model uses power as feature -> fix that in raw data\n",
        "data_processed = data_processed.rename(columns={'electricity': 'power'})\n",
        "# the processed data form bq needs to be converted to datetime object\n",
        "data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)\n",
        "\n",
        "if data_processed.shape[0] < 240:\n",
        "    print(\"‚ùå Not enough processed data retrieved to train on\")\n",
        "    # return None\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train = data_processed[data_processed['utc_time'] < '2020-01-01']\n",
        "test = data_processed[data_processed['utc_time'] >= '2020-01-01']\n",
        "\n",
        "train = train[['power']]\n",
        "test = test[['power']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rOxQQjEGM_wv"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = get_X_y_seq(train,\n",
        "                                number_of_sequences=10_000,\n",
        "                                input_length=48,\n",
        "                                output_length=24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIV2SlY9Rasb"
      },
      "source": [
        "# Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38EPazt5LcyS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import models, layers, optimizers, metrics\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def initialize_model(X_train, y_train, n_unit=24):\n",
        "\n",
        "    # 1 - RNN architecture\n",
        "    # ======================\n",
        "    model = models.Sequential()\n",
        "\n",
        "    ## 1.1 - Recurrent Layer\n",
        "    model.add(layers.LSTM(n_unit,\n",
        "                          activation='tanh',\n",
        "                          return_sequences = False,\n",
        "                          input_shape=(X_train.shape[1],X_train.shape[2])\n",
        "                          ))\n",
        "    ## 1.2 - Predictive Dense Layers\n",
        "    output_length = y_train.shape[1]\n",
        "    model.add(layers.Dense(output_length, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def compile_model(model, learning_rate=0.02):\n",
        "\n",
        "    # def r_squared(y_true, y_pred):\n",
        "    #     ss_res = K.sum(K.square(y_true - y_pred))\n",
        "    #     ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "    #     return (1 - ss_res/(ss_tot + K.epsilon()))\n",
        "\n",
        "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(loss='mse', optimizer=adam, metrics=['mae']) #, r_squared])\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model,\n",
        "                X_train,\n",
        "                y_train,\n",
        "                validation_split = 0.3,\n",
        "                batch_size = 32,\n",
        "                epochs = 50):\n",
        "    es = EarlyStopping(monitor = \"val_mae\",\n",
        "                       mode = \"min\",\n",
        "                       patience = 5,\n",
        "                       restore_best_weights = True)\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_split=validation_split,\n",
        "                        shuffle=False,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs,\n",
        "                        callbacks = [es],\n",
        "                        verbose = 0)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwAYdGD-RfTb"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc20G9SnNIqT"
      },
      "outputs": [],
      "source": [
        "model = initialize_model(X_train, y_train, n_unit=24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV6aQ_4hNbhP",
        "outputId": "be4c6c3f-1d62-4de4-e7be-8d3a48bbe346"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odHe3B4jNhC5"
      },
      "outputs": [],
      "source": [
        "model = compile_model(model, learning_rate=0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JifCWO3CNpbh"
      },
      "outputs": [],
      "source": [
        "model, history = train_model(model,\n",
        "                                X_train,\n",
        "                                y_train,\n",
        "                                validation_split = 0.3,\n",
        "                                batch_size = 32,\n",
        "                                epochs = 50\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igNpsFd5NuJF",
        "outputId": "788060b8-5a0f-4206-d506-5f428ba9e193"
      },
      "outputs": [],
      "source": [
        "val_mae = np.min(history.history['val_mae'])\n",
        "val_mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJa1p0VwNwPm",
        "outputId": "b4143b8a-f108-4e76-acee-6cff05721de9"
      },
      "outputs": [],
      "source": [
        "params = dict(\n",
        "    context=\"train\",\n",
        "    training_set_size='40 years worth of data',\n",
        "    row_count=len(X_train),\n",
        ")\n",
        "params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9G9Po6uQxuB"
      },
      "source": [
        "# Attempt to save & load model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(model: keras.Model = None) -> None:\n",
        "    \"\"\"\n",
        "    Persist trained model locally on the hard drive at f\"{LOCAL_REGISTRY_PATH}/models/{timestamp}.h5\"\n",
        "    - if MODEL_TARGET='gcs', also persist it in your bucket on GCS at \"models/{timestamp}.h5\" --> unit 02 only\n",
        "    - if MODEL_TARGET='mlflow', also persist it on MLflow instead of GCS (for unit 0703 only) --> unit 03 only\n",
        "    \"\"\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    # Save model locally\n",
        "    model_path = os.path.join(LOCAL_REGISTRY_PATH, \"models\", f\"{timestamp}.h5\")\n",
        "    model.save(model_path)\n",
        "\n",
        "    print(\"‚úÖ Model saved locally\")\n",
        "\n",
        "    if MODEL_TARGET == \"gcs\":\n",
        "        # üéÅ We give you this piece of code as a gift. Please read it carefully! Add a breakpoint if needed!\n",
        "        print(Fore.BLUE + f\"\\nSave model to GCS @ {BUCKET_NAME}...\" + Style.RESET_ALL)\n",
        "\n",
        "        model_filename = model_path.split(\"/\")[-1] # e.g. \"20230208-161047.h5\" for instance\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob(f\"models/{model_filename}\")\n",
        "        blob.upload_from_filename(model_path)\n",
        "\n",
        "        print(\"‚úÖ Model saved to GCS\")\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_TARGET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(stage=\"Production\") -> keras.Model:\n",
        "    \"\"\"\n",
        "    Return a saved model:\n",
        "    - locally (latest one in alphabetical order)\n",
        "    - or from GCS (most recent one) if MODEL_TARGET=='gcs'  --> for unit 02 only\n",
        "    - or from MLFLOW (by \"stage\") if MODEL_TARGET=='mlflow' --> for unit 03 only\n",
        "\n",
        "    Return None (but do not Raise) if no model is found\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if MODEL_TARGET == \"local\":\n",
        "        print(Fore.BLUE + f\"\\nLoad latest model from local registry...\" + Style.RESET_ALL)\n",
        "\n",
        "        # Get the latest model version name by the timestamp on disk\n",
        "        local_model_directory = os.path.join(LOCAL_REGISTRY_PATH, \"models\")\n",
        "        local_model_paths = glob.glob(f\"{local_model_directory}/*\")\n",
        "\n",
        "        if not local_model_paths:\n",
        "            return None\n",
        "\n",
        "        most_recent_model_path_on_disk = sorted(local_model_paths)[-1]\n",
        "\n",
        "        print(Fore.BLUE + f\"\\nLoad latest model from disk...\" + Style.RESET_ALL)\n",
        "\n",
        "        latest_model = keras.models.load_model(most_recent_model_path_on_disk)\n",
        "\n",
        "        print(\"‚úÖ Model loaded from local disk\")\n",
        "\n",
        "        return latest_model\n",
        "\n",
        "    elif MODEL_TARGET == \"gcs\":\n",
        "        # üéÅ We give you this piece of code as a gift. Please read it carefully! Add a breakpoint if needed!\n",
        "        print(Fore.BLUE + f\"\\nLoad latest model from GCS...\" + Style.RESET_ALL)\n",
        "\n",
        "        client = storage.Client()\n",
        "        blobs = list(client.get_bucket(BUCKET_NAME).list_blobs(prefix=\"model\"))\n",
        "\n",
        "        try:\n",
        "            latest_blob = max(blobs, key=lambda x: x.updated)\n",
        "            latest_model_path_to_save = os.path.join(LOCAL_REGISTRY_PATH, latest_blob.name)\n",
        "            latest_blob.download_to_filename(latest_model_path_to_save)\n",
        "\n",
        "            latest_model = keras.models.load_model(latest_model_path_to_save)\n",
        "\n",
        "            print(\"‚úÖ Latest model downloaded from cloud storage\")\n",
        "\n",
        "            return latest_model\n",
        "        except:\n",
        "            print(f\"\\n‚ùå No model found in GCS bucket {BUCKET_NAME}\")\n",
        "\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPKxa3OCSkqf",
        "outputId": "5b90266e-ff7b-4133-8a08-4e9b4a896f0f"
      },
      "outputs": [],
      "source": [
        "latest_model = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "latest_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Pred function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((10000, 48, 1), (10000, 24, 1))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pred(X_pred:str = '2013-05-08 12:00:00') -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Make a prediction using the latest trained model\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n‚≠êÔ∏è Use case: predict\")\n",
        "\n",
        "    # X_pred = datetime.strptime(X_pred, '%Y-%m-%d %H:%M:%S')\n",
        "    # reference_datetime = datetime.strptime(\"1980-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S')\n",
        "    # time_difference = X_pred - reference_datetime\n",
        "    # time_difference_hours = time_difference.total_seconds() / 3600\n",
        "    # input_date = X_test[time_difference_hours-47: time_difference_hours+1]\n",
        "\n",
        "\n",
        "\n",
        "    # if X_pred is None:\n",
        "    #     X_pred = pd.DataFrame(dict(\n",
        "    #     pickup_datetime=[pd.Timestamp(\"2013-07-06 17:18:00\", tz='UTC')],\n",
        "    #     pickup_longitude=[-73.950655],\n",
        "    #     pickup_latitude=[40.783282],\n",
        "    #     dropoff_longitude=[-73.984365],\n",
        "    #     dropoff_latitude=[40.769802],\n",
        "    #     passenger_count=[1],\n",
        "    # ))\n",
        "\n",
        "    model = load_model()\n",
        "    assert model is not None\n",
        "\n",
        "    # X_processed = preprocess_features(X_pred)\n",
        "    # y_pred = model.predict(X_processed)\n",
        "\n",
        "    # print(\"\\n‚úÖ prediction done: \", y_pred, y_pred.shape, \"\\n\")\n",
        "    print(\"\\n‚úÖ prediction done: \\n\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model.summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
