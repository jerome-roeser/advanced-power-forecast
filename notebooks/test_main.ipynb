{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1) imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from colorama import Fore, Style\n",
    "from dateutil.parser import parse\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "from datetime import datetime\n",
    "\n",
    "from power.params import *\n",
    "from power.ml_ops.data import *\n",
    "from power.ml_ops.model import *\n",
    "from power.ml_ops.registry import load_model, save_model, save_results\n",
    "from power.ml_ops.cross_val import get_X_y_seq, get_X_y_seq_pv\n",
    "from power.interface.main import evaluate, pred\n",
    "\n",
    "from power.utils import plot_loss_mae\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load saved sequences\n",
    "X = np.load('X_train.npy')\n",
    "y = np.load('y_train.npy')\n",
    "\n",
    "X_full = np.load('X_train_full.npy')\n",
    "y_full = np.load('y_train_full.npy')\n",
    "\n",
    "## Get all data\n",
    "data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed_pv = pd.read_csv(data_processed_pv_cache_path)\n",
    "\n",
    "data_processed_forecast_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_weather_forecast.csv\")\n",
    "data_processed_forecast = pd.read_csv(data_processed_forecast_cache_path)\n",
    "\n",
    "data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "stats_df = get_stats_table(data_processed_pv, capacity=False)\n",
    "pred_df = pred(input_pred='2022-07-06 12:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        min_date: str = '2017-10-07 00:00:00',\n",
    "        max_date: str = '2019-12-31 23:00:00',\n",
    "        split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set\n",
    "        learning_rate: float =0.02,\n",
    "        batch_size: int = 32,\n",
    "        patience: int = 5,\n",
    "        forecast_features: bool = False\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    - Download processed data from your BQ table (or from cache if it exists)\n",
    "    - Train on the preprocessed dataset (which should be ordered by date)\n",
    "    - Store training results and model weights\n",
    "\n",
    "    Return val_mae as a float\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
    "    print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "    # --First-- Load processed PV data using `get_data_with_cache` in chronological order\n",
    "    query_pv = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed_pv = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query_pv,\n",
    "        cache_path=data_processed_pv_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "    # the processed PV data from bq needs to be converted to datetime object\n",
    "    data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "\n",
    "    if data_processed_pv.shape[0] < 240:\n",
    "        print(\"❌ Not enough processed data retrieved to train on\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    if forecast_features:\n",
    "    # --Second-- Load processed Weather Forecast data in chronological order\n",
    "        query_forecast = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {GCP_PROJECT}.{BQ_DATASET}.processed_weather_forecast\n",
    "            ORDER BY forecast_dt_unixtime, slice_dt_unixtime\n",
    "        \"\"\"\n",
    "\n",
    "        data_processed_forecast_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_weather_forecast.csv\")\n",
    "        data_processed_forecast = get_data_with_cache(\n",
    "            gcp_project=GCP_PROJECT,\n",
    "            query=query_forecast,\n",
    "            cache_path=data_processed_forecast_cache_path,\n",
    "            data_has_header=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        if data_processed_forecast.shape[0] < 240:\n",
    "            print(\"❌ Not enough processed data retrieved to train on\")\n",
    "            return None\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "\n",
    "        train_pv = data_processed_pv[(data_processed_pv['utc_time'] > min_date) \\\n",
    "                                 & (data_processed_pv['utc_time'] < max_date)]\n",
    "        train_forecast = data_processed_forecast\n",
    "\n",
    "        X_train, y_train = get_X_y_seq(train_pv,\n",
    "                                    train_forecast,\n",
    "                                    number_of_sequences=10_000,\n",
    "                                    input_length=48,\n",
    "                                    output_length=24,\n",
    "                                    gap_hours=12)\n",
    "\n",
    "        # Train model using `model.py`\n",
    "        model = load_model(forecast_features=True)\n",
    "\n",
    "        if model is None:\n",
    "            model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = compile_model(model, learning_rate=learning_rate)\n",
    "        model, history = train_model(model,\n",
    "                                    X_train,\n",
    "                                    y_train,\n",
    "                                    validation_split = 0.3,\n",
    "                                    batch_size = 32,\n",
    "                                    epochs = 50\n",
    "                                    )\n",
    "        val_mae = np.min(history.history['val_mae'])\n",
    "\n",
    "        params = dict(\n",
    "            context=\"train\",\n",
    "            training_set_size=f'Training data from {min_date} to {max_date}',\n",
    "            row_count=len(X_train),\n",
    "        )\n",
    "\n",
    "        # Save results on the hard drive using taxifare.ml_logic.registry\n",
    "        save_results(params=params, metrics=dict(mae=val_mae))\n",
    "\n",
    "        # Save model weight on the hard drive (and optionally on GCS too!)\n",
    "        save_model(model=model, forecast_features= True)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        train_pv = data_processed_pv[data_processed_pv['utc_time'] < max_date]\n",
    "\n",
    "        X_train, y_train = get_X_y_seq_pv(train_pv,\n",
    "                                    number_of_sequences=10_000,\n",
    "                                    input_length=48,\n",
    "                                    output_length=24,\n",
    "                                    gap_hours=12)\n",
    "\n",
    "        # Train model using `model.py`\n",
    "        model = load_model()\n",
    "\n",
    "        if model is None:\n",
    "            model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = compile_model(model, learning_rate=learning_rate)\n",
    "        model, history = train_model(model,\n",
    "                                    X_train,\n",
    "                                    y_train,\n",
    "                                    validation_split = 0.3,\n",
    "                                    batch_size = 32,\n",
    "                                    epochs = 50\n",
    "                                    )\n",
    "\n",
    "        val_mae = np.min(history.history['val_mae'])\n",
    "\n",
    "        params = dict(\n",
    "            context=\"train\",\n",
    "            training_set_size=f'Training data from {min_date} to {max_date}',\n",
    "            row_count=len(X_train),\n",
    "        )\n",
    "\n",
    "        # Save results on the hard drive using power.ml_logic.registry\n",
    "        save_results(params=params, metrics=dict(mae=val_mae), history=history)\n",
    "\n",
    "        # Save model weight on the hard drive (and optionally on GCS too!)\n",
    "        save_model(model=model)\n",
    "\n",
    "    print(\"✅ train() done \\n\")\n",
    "\n",
    "    return X_train, y_train, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train, model, history = train(forecast_features= False)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file= 'X_train', arr= X_train)\n",
    "# np.save(file= 'y_train', arr= y_train)\n",
    "\n",
    "X = np.load('X_train.npy')\n",
    "y = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full , y_train_full, model_full, history_full = train(forecast_features= True)\n",
    "X_train_full.shape, y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file= 'X_train_full', arr= X_train_full)\n",
    "# np.save(file= 'y_train_full', arr= y_train_full)\n",
    "\n",
    "X_full = np.load('X_train_full.npy')\n",
    "y_full = np.load('y_train_full.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) historical PV production data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_mae(history), plot_loss_mae(history_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X, y)\n",
    "model = compile_model(model, learning_rate= 1e-3)\n",
    "model, history = train_model(model, X, y)\n",
    "plot_loss_mae(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) historical PV production + historical weather-forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_full, y_full)\n",
    "model = compile_model(model, learning_rate=1e-3)\n",
    "model, history = train_model(model, X_full, y_full)\n",
    "plot_loss_mae(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_data(input_date: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Return a numpy array for 3-days (before and ahead of sell date) statistics\n",
    "    of PV power prodcution for a sell date.\n",
    "\n",
    "    Input:\n",
    "    the sell date: 'YYYY-MM-DD' e.g. '2022-07-06'\n",
    "\n",
    "    Output:\n",
    "    numpy array of shape (72,11) for 72 hours of data with 11 features\n",
    "    Features:\n",
    "        {0:'utc_time', 1:'local_time', 2:'electricity', 3:'hour_of_year',\n",
    "        4:'mean', 5:'median', 6:'std', 7:'skew', 8'min', 9'max', 10:'count'}\n",
    "    \"\"\"\n",
    "    # collect input for postprocess\n",
    "    data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed_pv = pd.read_csv(data_processed_pv_cache_path)\n",
    "\n",
    "    data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "    stats_df = get_stats_table(data_processed_pv, capacity=False)\n",
    "\n",
    "    # get plot_df\n",
    "    plot_df = postprocess(input_date, data_processed_pv, stats_df)\n",
    "\n",
    "    # Send as dict from backend to frontend; NaNs have to be replaced\n",
    "    plot_df = plot_df.fillna(0.0)\n",
    "    # plot_dict = plot_df.to_dict()\n",
    "\n",
    "    return plot_df\n",
    "\n",
    "baseline_df = get_baseline_data(input_date= '2022-01-01')\n",
    "baseline_array = baseline_df.to_numpy()\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_array[-24:,4] # mean of day-ahead for keras lamdbda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_array[:24, 2] # day before sell for keras lambda layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### API call ==================================================================\n",
    "#base_url = \"http://127.0.0.1:8000\"\n",
    "base_url = \"https://power-v2-pdymu2v2na-ew.a.run.app\"\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def call_visu(today_date):\n",
    "\n",
    "    params_visu ={\n",
    "        'input_date': today_date,   # '2000-05-15' (dt.date())\n",
    "        'power_source': 'pv',\n",
    "        'capacity': 'true'\n",
    "        }\n",
    "\n",
    "    endpoint_visu = \"/visualisation\"\n",
    "    url_visu = f\"{base_url}{endpoint_visu}\"\n",
    "    response_visu = requests.get(url_visu, params_visu).json()\n",
    "\n",
    "    plot_df = pd.DataFrame.from_dict(response_visu)\n",
    "    plot_df.utc_time = pd.to_datetime(plot_df.utc_time,utc=True)\n",
    "\n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = call_visu('2022-12-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['min'].plot()\n",
    "plot_df['max'].plot()\n",
    "plot_df['mean'].plot()\n",
    "plot_df['cap_fac'].plot()\n",
    "plot_df['pred'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualisation(input_date: str, power_source='pv', capacity='false') -> None:\n",
    "  \"\"\"\n",
    "  input_date corresponds to \"today\"\n",
    "  \"\"\"\n",
    "\n",
    "  # collect input for postprocess\n",
    "  pred_df = pred( f\"{input_date} 12:00:00\")\n",
    "  data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "  preprocessed_df = pd.read_csv(data_processed_pv_cache_path)\n",
    "  preprocessed_df.utc_time = pd.to_datetime(preprocessed_df.utc_time,utc=True)\n",
    "\n",
    "  if capacity == 'true':\n",
    "    print('Capacity!')\n",
    "    preprocessed_df['cap_fac'] = preprocessed_df.electricity / 0.9 * 100 # 0.9 is max value for pv\n",
    "    stats_df = get_stats_table(preprocessed_df, capacity=True)\n",
    "    pred_df.pred = pred_df.pred / 0.9 * 100\n",
    "  else:\n",
    "    print('Electricity!')\n",
    "    stats_df = get_stats_table(preprocessed_df, capacity=False)\n",
    "\n",
    "  # get plot_df\n",
    "  plot_df = postprocess(input_date, preprocessed_df, stats_df, pred_df)\n",
    "\n",
    "  # Send as dict from backend to frontend; NaNs have to be replaced\n",
    "  plot_df = plot_df.fillna(0.0)\n",
    "  plot_dict = plot_df.to_dict()\n",
    "\n",
    "  return plot_dict\n",
    "\n",
    "plot_dict = visualisation('2022-07-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "\n",
    "# used in the plots\n",
    "today_date = st.session_state['today']\n",
    "plot_df = st.session_state['plot_df']\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "### capacity\n",
    "\n",
    "# time variables\n",
    "today_dt = pd.Timestamp(today_date, tz='UTC')\n",
    "time = plot_df.utc_time.values\n",
    "\n",
    "sep_future = today_dt + pd.Timedelta(days=1)\n",
    "sep_past = today_dt\n",
    "sep_order = today_dt + pd.Timedelta(hours=12)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ax.axvline(sep_past, color='k', linewidth=0.7)\n",
    "ax.axvline(sep_future, color='k', linewidth=0.7)\n",
    "ax.vlines(sep_order, ymin=0, ymax=100, color='k', linewidth=0.7, linestyle='--')\n",
    "\n",
    "# stats\n",
    "alpha_stats = 0.2\n",
    "ax.step(time, plot_df['min'].values, where='pre',\n",
    "        color='k', linestyle=':', alpha=alpha_stats, label='min')\n",
    "ax.step(time, plot_df['max'].values, where='pre',\n",
    "        color='k', linestyle=':', alpha=alpha_stats, label='max')\n",
    "ax.step(time, plot_df['mean'].values, where='pre',\n",
    "        color='k', linestyle='-', alpha=alpha_stats, label='mean')\n",
    "\n",
    "lower_bound = plot_df['mean'].values - 1 * plot_df['std'].values\n",
    "upper_bound = plot_df['mean'].values + 1 * plot_df['std'].values\n",
    "ax.fill_between(time, lower_bound, upper_bound, step='pre',\n",
    "                color='gray',\n",
    "                alpha=alpha_stats,\n",
    "                label='std')\n",
    "\n",
    "# true current production data\n",
    "current = 37 # current production data\n",
    "ax.step(time[:current], plot_df.cap_fac.values[:current], where='pre',\n",
    "        color='orange', linewidth=4, label='true')\n",
    "\n",
    "# prediction day ahead data\n",
    "hori = -24\n",
    "ax.step(time[hori:], plot_df.pred.values[hori:], where='pre',\n",
    "        color='orange', linewidth=4, linestyle=':', label='pred')\n",
    "\n",
    "###\n",
    "if show_true == 'Yes':\n",
    "    ax.step(time[-36:], plot_df.cap_fac.values[-36:], where='pre',\n",
    "         color='orange', linewidth=4, linestyle='-', alpha=0.4)\n",
    "    st.sidebar.write('')\n",
    "else:\n",
    "    st.sidebar.write('')\n",
    "\n",
    "# date ticks\n",
    "ax.xaxis.set_major_locator(dates.HourLocator(byhour=range(24), interval=12, tz='UTC'))\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%H:%M %d/%m/%Y'))\n",
    "\n",
    "ax.set_xlim(today_dt - pd.Timedelta(days=1), today_dt + pd.Timedelta(days=2))\n",
    "ax.set_ylim(0,120.0)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Capacity factor in %')\n",
    "\n",
    "ax.annotate('Day-Ahead',(0.77,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Today',(0.48,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Day-Behind',(0.15,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Order book closed',(0.51,0.77), xycoords='subfigure fraction')\n",
    "#ax.set_title(f\"Day Ahead prediction for { sep_future.strftime('%d/%m/%Y') }\")\n",
    "\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-Shvux39R-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
