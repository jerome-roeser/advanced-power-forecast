{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1) imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 08:36:06.610132: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 08:36:07.190693: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 08:36:07.193882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-19 08:36:24.054023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from colorama import Fore, Style\n",
    "from dateutil.parser import parse\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "from datetime import datetime\n",
    "\n",
    "from power.params import *\n",
    "from power.ml_ops.data import *\n",
    "from power.ml_ops.model import *\n",
    "from power.ml_ops.registry import load_model, save_model, save_results\n",
    "from power.ml_ops.cross_val import get_X_y_seq, get_X_y_seq_pv\n",
    "from power.interface.main import train, evaluate, pred\n",
    "\n",
    "from power.utils import plot_loss_mae\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/advanced-power-forecast/power/ml_ops/data.py:272: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  years_df['hour_of_year'] = years_df.utc_time.\\\n"
     ]
    }
   ],
   "source": [
    "#### Load saved sequences\n",
    "sequence_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"sequences\")\n",
    "X = np.load(sequence_cache_path.joinpath('X_train.npy'))\n",
    "y = np.load(sequence_cache_path.joinpath('y_train.npy'))\n",
    "\n",
    "X_full = np.load(sequence_cache_path.joinpath('X_train_full.npy'))\n",
    "y_full = np.load(sequence_cache_path.joinpath('y_train_full.npy'))\n",
    "\n",
    "## Get all data\n",
    "data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed_pv = pd.read_csv(data_processed_pv_cache_path)\n",
    "\n",
    "data_processed_forecast_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_weather_forecast.csv\")\n",
    "data_processed_forecast = pd.read_csv(data_processed_forecast_cache_path)\n",
    "\n",
    "\n",
    "data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "\n",
    "min_date_pv= '1980-01-01 00:00:00'\n",
    "min_date_forecast = '2017-10-07 00:00:00'\n",
    "max_date = '2019-12-31 23:00:00'\n",
    "train_pv = data_processed_pv[(data_processed_pv['utc_time'] > min_date_forecast) \\\n",
    "                                 & (data_processed_pv['utc_time'] < max_date)]\n",
    "\n",
    "train_forecast = data_processed_forecast\n",
    "\n",
    "stats_df = get_stats_table(data_processed_pv, capacity=False)\n",
    "# pred_df = pred(input_pred='2022-07-06 12:00:00', forecast_features= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "⭐️ Use case: predict\u001b[0m\n",
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (376944, 6)\n",
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (91704, 24)\n",
      "\u001b[34m\n",
      "Predict with Index(['electricity', 'day_sin', 'day_cos', 'year_sin', 'year_cos'], dtype='object') data_processed_pv (forecast features: True)...:\u001b[0m\n",
      "\u001b[34m\n",
      "Predict with Index(['temperature', 'dew_point', 'pressure', 'ground_pressure', 'humidity',\n",
      "       'clouds', 'rain', 'snow', 'ice', 'fr_rain', 'convective', 'snow_depth',\n",
      "       'accumulated', 'hours', 'rate', 'probability', 'day_sin', 'day_cos',\n",
      "       'year_sin', 'year_cos', 'Wx', 'Wy'],\n",
      "      dtype='object') data_processed_pv (forecast features: True)...:\u001b[0m\n",
      "\u001b[34m\n",
      "Load latest model from local registry...\u001b[0m\n",
      "\u001b[34m\n",
      "Load latest model from disk...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 08:51:53.220849: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-11-19 08:51:53.228753: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-11-19 08:51:53.238269: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded from local disk\n",
      "\u001b[34m\n",
      "Predict with (1, 48, 27) X_pred tensor (forecast features: True)...:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 08:51:54.201390: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-11-19 08:51:54.206492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-11-19 08:51:54.212540: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x776ea6346cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "\n",
      "✅ prediction done:  [[0.13695635 0.1312954  0.12576167 0.12601697 0.1138956  0.09971845\n",
      "  0.10265615 0.10753613 0.11267563 0.11922325 0.11989239 0.11581486\n",
      "  0.12090086 0.12884232 0.1338112  0.13500968 0.14266115 0.13608047\n",
      "  0.13459432 0.12228379 0.13395695 0.13446927 0.14005269 0.15244603]] (1, 24) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utc_time</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-07-07 00:00:00+00:00</td>\n",
       "      <td>0.136956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-07-07 01:00:00+00:00</td>\n",
       "      <td>0.131295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-07-07 02:00:00+00:00</td>\n",
       "      <td>0.125762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-07-07 03:00:00+00:00</td>\n",
       "      <td>0.126017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-07-07 04:00:00+00:00</td>\n",
       "      <td>0.113896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-07-07 05:00:00+00:00</td>\n",
       "      <td>0.099718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-07-07 06:00:00+00:00</td>\n",
       "      <td>0.102656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-07-07 07:00:00+00:00</td>\n",
       "      <td>0.107536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-07-07 08:00:00+00:00</td>\n",
       "      <td>0.112676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-07-07 09:00:00+00:00</td>\n",
       "      <td>0.119223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-07-07 10:00:00+00:00</td>\n",
       "      <td>0.119892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-07-07 11:00:00+00:00</td>\n",
       "      <td>0.115815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-07-07 12:00:00+00:00</td>\n",
       "      <td>0.120901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-07-07 13:00:00+00:00</td>\n",
       "      <td>0.128842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-07-07 14:00:00+00:00</td>\n",
       "      <td>0.133811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-07-07 15:00:00+00:00</td>\n",
       "      <td>0.135010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-07-07 16:00:00+00:00</td>\n",
       "      <td>0.142661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-07-07 17:00:00+00:00</td>\n",
       "      <td>0.136080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-07-07 18:00:00+00:00</td>\n",
       "      <td>0.134594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-07-07 19:00:00+00:00</td>\n",
       "      <td>0.122284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-07-07 20:00:00+00:00</td>\n",
       "      <td>0.133957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-07-07 21:00:00+00:00</td>\n",
       "      <td>0.134469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-07-07 22:00:00+00:00</td>\n",
       "      <td>0.140053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-07-07 23:00:00+00:00</td>\n",
       "      <td>0.152446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    utc_time      pred\n",
       "0  2022-07-07 00:00:00+00:00  0.136956\n",
       "1  2022-07-07 01:00:00+00:00  0.131295\n",
       "2  2022-07-07 02:00:00+00:00  0.125762\n",
       "3  2022-07-07 03:00:00+00:00  0.126017\n",
       "4  2022-07-07 04:00:00+00:00  0.113896\n",
       "5  2022-07-07 05:00:00+00:00  0.099718\n",
       "6  2022-07-07 06:00:00+00:00  0.102656\n",
       "7  2022-07-07 07:00:00+00:00  0.107536\n",
       "8  2022-07-07 08:00:00+00:00  0.112676\n",
       "9  2022-07-07 09:00:00+00:00  0.119223\n",
       "10 2022-07-07 10:00:00+00:00  0.119892\n",
       "11 2022-07-07 11:00:00+00:00  0.115815\n",
       "12 2022-07-07 12:00:00+00:00  0.120901\n",
       "13 2022-07-07 13:00:00+00:00  0.128842\n",
       "14 2022-07-07 14:00:00+00:00  0.133811\n",
       "15 2022-07-07 15:00:00+00:00  0.135010\n",
       "16 2022-07-07 16:00:00+00:00  0.142661\n",
       "17 2022-07-07 17:00:00+00:00  0.136080\n",
       "18 2022-07-07 18:00:00+00:00  0.134594\n",
       "19 2022-07-07 19:00:00+00:00  0.122284\n",
       "20 2022-07-07 20:00:00+00:00  0.133957\n",
       "21 2022-07-07 21:00:00+00:00  0.134469\n",
       "22 2022-07-07 22:00:00+00:00  0.140053\n",
       "23 2022-07-07 23:00:00+00:00  0.152446"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(forecast_features= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        min_date: str = '2017-10-07 00:00:00',\n",
    "        max_date: str = '2019-12-31 23:00:00',\n",
    "        split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set\n",
    "        learning_rate: float =0.02,\n",
    "        batch_size: int = 32,\n",
    "        patience: int = 5,\n",
    "        forecast_features: bool = False\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    - Download processed data from your BQ table (or from cache if it exists)\n",
    "    - Train on the preprocessed dataset (which should be ordered by date)\n",
    "    - Store training results and model weights\n",
    "\n",
    "    Return val_mae as a float\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
    "    print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "    # --First-- Load processed PV data using `get_data_with_cache` in chronological order\n",
    "    query_pv = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed_pv = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query_pv,\n",
    "        cache_path=data_processed_pv_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "    # the processed PV data from bq needs to be converted to datetime object\n",
    "    data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "\n",
    "    if data_processed_pv.shape[0] < 240:\n",
    "        print(\"❌ Not enough processed data retrieved to train on\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    if forecast_features:\n",
    "    # --Second-- Load processed Weather Forecast data in chronological order\n",
    "        query_forecast = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {GCP_PROJECT}.{BQ_DATASET}.processed_weather_forecast\n",
    "            ORDER BY forecast_dt_unixtime, slice_dt_unixtime\n",
    "        \"\"\"\n",
    "\n",
    "        data_processed_forecast_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_weather_forecast.csv\")\n",
    "        data_processed_forecast = get_data_with_cache(\n",
    "            gcp_project=GCP_PROJECT,\n",
    "            query=query_forecast,\n",
    "            cache_path=data_processed_forecast_cache_path,\n",
    "            data_has_header=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        if data_processed_forecast.shape[0] < 240:\n",
    "            print(\"❌ Not enough processed data retrieved to train on\")\n",
    "            return None\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "\n",
    "        train_pv = data_processed_pv[(data_processed_pv['utc_time'] > min_date) \\\n",
    "                                 & (data_processed_pv['utc_time'] < max_date)]\n",
    "        train_forecast = data_processed_forecast\n",
    "\n",
    "        X_train, y_train = get_X_y_seq(train_pv,\n",
    "                                    train_forecast,\n",
    "                                    number_of_sequences=10_000,\n",
    "                                    input_length=48,\n",
    "                                    output_length=24,\n",
    "                                    gap_hours=12)\n",
    "\n",
    "        # Train model using `model.py`\n",
    "        model = load_model(forecast_features=True)\n",
    "\n",
    "        if model is None:\n",
    "            model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = compile_model(model, learning_rate=learning_rate)\n",
    "        model, history = train_model(model,\n",
    "                                    X_train,\n",
    "                                    y_train,\n",
    "                                    validation_split = 0.3,\n",
    "                                    batch_size = 32,\n",
    "                                    epochs = 50\n",
    "                                    )\n",
    "        val_mae = np.min(history.history['val_mae'])\n",
    "\n",
    "        params = dict(\n",
    "            context=\"train\",\n",
    "            training_set_size=f'Training data from {min_date} to {max_date}',\n",
    "            row_count=len(X_train),\n",
    "        )\n",
    "\n",
    "        # Save results on the hard drive using taxifare.ml_logic.registry\n",
    "        save_results(params=params, metrics=dict(mae=val_mae))\n",
    "\n",
    "        # Save model weight on the hard drive (and optionally on GCS too!)\n",
    "        save_model(model=model, forecast_features= True)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        train_pv = data_processed_pv[data_processed_pv['utc_time'] < max_date]\n",
    "\n",
    "        X_train, y_train = get_X_y_seq_pv(train_pv,\n",
    "                                    number_of_sequences=10_000,\n",
    "                                    input_length=48,\n",
    "                                    output_length=24,\n",
    "                                    gap_hours=12)\n",
    "\n",
    "        # Train model using `model.py`\n",
    "        model = load_model()\n",
    "\n",
    "        if model is None:\n",
    "            model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "        model = compile_model(model, learning_rate=learning_rate)\n",
    "        model, history = train_model(model,\n",
    "                                    X_train,\n",
    "                                    y_train,\n",
    "                                    validation_split = 0.3,\n",
    "                                    batch_size = 32,\n",
    "                                    epochs = 50\n",
    "                                    )\n",
    "\n",
    "        val_mae = np.min(history.history['val_mae'])\n",
    "\n",
    "        params = dict(\n",
    "            context=\"train\",\n",
    "            training_set_size=f'Training data from {min_date} to {max_date}',\n",
    "            row_count=len(X_train),\n",
    "        )\n",
    "\n",
    "        # Save results on the hard drive using power.ml_logic.registry\n",
    "        save_results(params=params, metrics=dict(mae=val_mae), history=history)\n",
    "\n",
    "        # Save model weight on the hard drive (and optionally on GCS too!)\n",
    "        save_model(model=model)\n",
    "\n",
    "    print(\"✅ train() done \\n\")\n",
    "\n",
    "    return X_train, y_train, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train, model, history = train(forecast_features= False)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file= 'X_train', arr= X_train)\n",
    "# np.save(file= 'y_train', arr= y_train)\n",
    "\n",
    "X = np.load('X_train.npy')\n",
    "y = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full , y_train_full, model_full, history_full = train(forecast_features= True)\n",
    "X_train_full.shape, y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file= 'X_train_full', arr= X_train_full)\n",
    "# np.save(file= 'y_train_full', arr= y_train_full)\n",
    "\n",
    "X_full = np.load('X_train_full.npy')\n",
    "y_full = np.load('y_train_full.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) historical PV production data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_mae(history), plot_loss_mae(history_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X, y)\n",
    "model = compile_model(model, learning_rate= 1e-3)\n",
    "model, history = train_model(model, X, y)\n",
    "plot_loss_mae(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) historical PV production + historical weather-forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_full, y_full)\n",
    "model = compile_model(model, learning_rate=1e-3)\n",
    "model, history = train_model(model, X_full, y_full)\n",
    "plot_loss_mae(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_data(input_date: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Return a numpy array for 3-days (before and ahead of sell date) statistics\n",
    "    of PV power prodcution for a sell date.\n",
    "\n",
    "    Input:\n",
    "    the sell date: 'YYYY-MM-DD' e.g. '2022-07-06'\n",
    "\n",
    "    Output:\n",
    "    numpy array of shape (72,11) for 72 hours of data with 11 features\n",
    "    Features:\n",
    "        {0:'utc_time', 1:'local_time', 2:'electricity', 3:'hour_of_year',\n",
    "        4:'mean', 5:'median', 6:'std', 7:'skew', 8'min', 9'max', 10:'count'}\n",
    "    \"\"\"\n",
    "    # collect input for postprocess\n",
    "    data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed_pv = pd.read_csv(data_processed_pv_cache_path)\n",
    "\n",
    "    data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "    stats_df = get_stats_table(data_processed_pv, capacity=False)\n",
    "\n",
    "    # get plot_df\n",
    "    plot_df = postprocess(input_date, data_processed_pv, stats_df)\n",
    "\n",
    "    # Send as dict from backend to frontend; NaNs have to be replaced\n",
    "    plot_df = plot_df.fillna(0.0)\n",
    "    # plot_dict = plot_df.to_dict()\n",
    "\n",
    "    return plot_df\n",
    "\n",
    "baseline_df = get_baseline_data(input_date= '2022-01-01')\n",
    "baseline_array = baseline_df.to_numpy()\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_array[-24:,4] # mean of day-ahead for keras lamdbda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_array[:24, 2] # day before sell for keras lambda layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### API call ==================================================================\n",
    "#base_url = \"http://127.0.0.1:8000\"\n",
    "base_url = \"https://power-v2-pdymu2v2na-ew.a.run.app\"\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def call_visu(today_date):\n",
    "\n",
    "    params_visu ={\n",
    "        'input_date': today_date,   # '2000-05-15' (dt.date())\n",
    "        'power_source': 'pv',\n",
    "        'capacity': 'true'\n",
    "        }\n",
    "\n",
    "    endpoint_visu = \"/visualisation\"\n",
    "    url_visu = f\"{base_url}{endpoint_visu}\"\n",
    "    response_visu = requests.get(url_visu, params_visu).json()\n",
    "\n",
    "    plot_df = pd.DataFrame.from_dict(response_visu)\n",
    "    plot_df.utc_time = pd.to_datetime(plot_df.utc_time,utc=True)\n",
    "\n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as dates\n",
    "\n",
    "\n",
    "# used in the plots\n",
    "today_date = '2022-07-10'\n",
    "plot_df = call_visu(today_date)\n",
    "\n",
    "#------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_true = 'No'\n",
    "\n",
    "### capacity\n",
    "\n",
    "# time variables\n",
    "today_dt = pd.Timestamp(today_date, tz='UTC')\n",
    "time = plot_df.utc_time.values\n",
    "\n",
    "sep_future = today_dt + pd.Timedelta(days=1)\n",
    "sep_past = today_dt\n",
    "sep_order = today_dt + pd.Timedelta(hours=12)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ax.axvline(sep_past, color='k', linewidth=0.7)\n",
    "ax.axvline(sep_future, color='k', linewidth=0.7)\n",
    "ax.vlines(sep_order, ymin=0, ymax=100, color='k', linewidth=0.7, linestyle='--')\n",
    "\n",
    "# stats\n",
    "# alpha_stats = 0.2\n",
    "# ax.step(time, plot_df['min'].values, where='pre',\n",
    "#         color='k', linestyle=':', alpha=alpha_stats, label='min')\n",
    "# ax.step(time, plot_df['max'].values, where='pre',\n",
    "#         color='k', linestyle=':', alpha=alpha_stats, label='max')\n",
    "# ax.step(time, plot_df['mean'].values, where='pre',\n",
    "#         color='k', linestyle='-', alpha=alpha_stats, label='mean')\n",
    "\n",
    "# lower_bound = plot_df['mean'].values - 1 * plot_df['std'].values\n",
    "# upper_bound = plot_df['mean'].values + 1 * plot_df['std'].values\n",
    "# ax.fill_between(time, lower_bound, upper_bound, step='pre',\n",
    "#                 color='gray',\n",
    "#                 alpha=alpha_stats,\n",
    "#                 label='std')\n",
    "\n",
    "# true current production data\n",
    "current = 37 # current production data\n",
    "ax.step(time[:current], plot_df.cap_fac.values[:current], where='pre',\n",
    "        color='orange', linewidth=4, label='true')\n",
    "\n",
    "# prediction day ahead data\n",
    "hori = -24\n",
    "ax.step(time[hori:], plot_df.pred.values[hori:], where='pre',\n",
    "        color='orange', linewidth=4, linestyle=':', label='pred')\n",
    "\n",
    "###\n",
    "if show_true == 'Yes':\n",
    "    ax.step(time[-36:], plot_df.cap_fac.values[-36:], where='pre',\n",
    "         color='orange', linewidth=4, linestyle='-', alpha=0.4)\n",
    "\n",
    "# date ticks\n",
    "ax.xaxis.set_major_locator(dates.HourLocator(byhour=range(24), interval=6, tz='UTC'))\n",
    "# ax.xaxis.set_major_formatter(dates.DateFormatter('%H:%M %d/%m/%Y'))\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%H:%M'))\n",
    "\n",
    "ax.set_xlim(today_dt - pd.Timedelta(days=1), today_dt + pd.Timedelta(days=2))\n",
    "ax.set_ylim(0,120.0)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Capacity factor in %')\n",
    "\n",
    "ax.annotate('Day-Ahead',(0.77,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Today',(0.48,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Day-Behind',(0.15,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Order book closed',(0.52,0.77), xycoords='subfigure fraction')\n",
    "#ax.set_title(f\"Day Ahead prediction for { sep_future.strftime('%d/%m/%Y') }\")\n",
    "\n",
    "# ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-Shvux39R-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
