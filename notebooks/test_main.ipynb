{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1) imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 13:10:28.950966: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-14 13:10:29.135492: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-14 13:10:29.140562: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-14 13:10:32.253927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "from colorama import Fore, Style\n",
    "from dateutil.parser import parse\n",
    "from typing import Dict, List, Tuple, Sequence\n",
    "from datetime import datetime\n",
    "\n",
    "from power.params import *\n",
    "from power.ml_ops.data import *\n",
    "from power.ml_ops.model import *\n",
    "from power.ml_ops.registry import load_model, save_model, save_results\n",
    "from power.ml_ops.cross_val import get_X_y_seq, get_X_y_seq_pv\n",
    "from power.interface.main import pred\n",
    "\n",
    "from power.utils import plot_loss_mae\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/advanced-power-forecast/power/ml_ops/data.py:268: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  years_df['hour_of_year'] = years_df.utc_time.\\\n"
     ]
    }
   ],
   "source": [
    "#### Load saved sequences\n",
    "X = np.load('X_train.npy')\n",
    "y = np.load('y_train.npy')\n",
    "\n",
    "X_full = np.load('X_train_full.npy')\n",
    "y_full = np.load('y_train_full.npy')\n",
    "\n",
    "## Get all data\n",
    "data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed_pv = pd.read_csv(data_processed_pv_cache_path)\n",
    "\n",
    "data_processed_forecast_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_weather_forecast.csv\")\n",
    "data_processed_forecast = pd.read_csv(data_processed_forecast_cache_path)\n",
    "\n",
    "data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "stats_df = get_stats_table(data_processed_pv, capacity=False)\n",
    "# pred_df = pred(input_pred='2022-07-06 12:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  1.62299995e+01,  1.00000000e+02,\n",
       "         0.00000000e+00,  3.80999994e+00],\n",
       "       [ 0.00000000e+00,  3.30000013e-01,  7.40000000e+01,\n",
       "         0.00000000e+00,  5.09000015e+00],\n",
       "       [ 5.72000000e-01,  1.34499998e+01,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.15000010e+00],\n",
       "       [ 4.00000000e-03,  5.80000019e+00,  9.90000000e+01,\n",
       "         0.00000000e+00,  6.28999996e+00],\n",
       "       [ 7.30000000e-02, -6.70000017e-01,  3.00000000e+00,\n",
       "         0.00000000e+00,  2.29999995e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_full[:5,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        min_date = '2017-10-07 00:00:00',\n",
    "        max_date = '2019-12-31 23:00:00',\n",
    "        split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set\n",
    "        learning_rate=0.02,\n",
    "        batch_size = 32,\n",
    "        patience = 5,\n",
    "        forecast_features = False\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    - Download processed data from your BQ table (or from cache if it exists)\n",
    "    - Train on the preprocessed dataset (which should be ordered by date)\n",
    "    - Store training results and model weights\n",
    "\n",
    "    Return val_mae as a float\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
    "    print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "    # --First-- Load processed PV data using `get_data_with_cache` in chronological order\n",
    "    query_pv = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed_pv = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query_pv,\n",
    "        cache_path=data_processed_pv_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "    # the processed PV data from bq needs to be converted to datetime object\n",
    "    data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "\n",
    "    if data_processed_pv.shape[0] < 240:\n",
    "        print(\"❌ Not enough processed data retrieved to train on\")\n",
    "        return None\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_pv = data_processed_pv[(data_processed_pv['utc_time'] > min_date) \\\n",
    "                                 & (data_processed_pv['utc_time'] < max_date)]\n",
    "\n",
    "\n",
    "    if forecast_features:\n",
    "    # --Second-- Load processed Weather Forecast data in chronological order\n",
    "        query_forecast = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {GCP_PROJECT}.{BQ_DATASET}.processed_weather_forecast\n",
    "            ORDER BY forecast_dt_unixtime, slice_dt_unixtime\n",
    "        \"\"\"\n",
    "\n",
    "        data_processed_forecast_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_weather_forecast.csv\")\n",
    "        data_processed_forecast = get_data_with_cache(\n",
    "            gcp_project=GCP_PROJECT,\n",
    "            query=query_forecast,\n",
    "            cache_path=data_processed_forecast_cache_path,\n",
    "            data_has_header=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        if data_processed_forecast.shape[0] < 240:\n",
    "            print(\"❌ Not enough processed data retrieved to train on\")\n",
    "            return None\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        train_forecast = data_processed_forecast\n",
    "\n",
    "        X_train, y_train = get_X_y_seq(train_pv,\n",
    "                                    train_forecast,\n",
    "                                    number_of_sequences=10_000,\n",
    "                                    input_length=48,\n",
    "                                    output_length=24,\n",
    "                                    gap_hours=12)\n",
    "\n",
    "    else:\n",
    "        X_train, y_train = get_X_y_seq_pv(train_pv,\n",
    "                                    number_of_sequences=10_000,\n",
    "                                    input_length=48,\n",
    "                                    output_length=24,\n",
    "                                    gap_hours=12)\n",
    "\n",
    "    # Train model using `model.py`\n",
    "    model = load_model()\n",
    "\n",
    "    if model is None:\n",
    "        model = initialize_model(X_train, y_train, n_unit=24)\n",
    "\n",
    "\n",
    "    model = compile_model(model, learning_rate=learning_rate)\n",
    "    model, history = train_model(model,\n",
    "                                X_train,\n",
    "                                y_train,\n",
    "                                validation_split = 0.3,\n",
    "                                batch_size = 32,\n",
    "                                epochs = 50\n",
    "                                )\n",
    "\n",
    "    val_mae = np.min(history.history['val_mae'])\n",
    "\n",
    "    params = dict(\n",
    "        context=\"train\",\n",
    "        training_set_size=f'Training data from {min_date} to {max_date}',\n",
    "        row_count=len(X_train),\n",
    "    )\n",
    "\n",
    "    # Save results on the hard drive using taxifare.ml_logic.registry\n",
    "    save_results(params=params, metrics=dict(mae=val_mae))\n",
    "\n",
    "    # Save model weight on the hard drive (and optionally on GCS too!)\n",
    "    save_model(model=model)\n",
    "\n",
    "    print(\"✅ train() done \\n\")\n",
    "\n",
    "    return X_train , y_train, model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train , y_train, model, history = train(forecast_features= False)\n",
    "# X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file= 'X_train', arr= X_train)\n",
    "# np.save(file= 'y_train', arr= y_train)\n",
    "\n",
    "X = np.load('X_train.npy')\n",
    "y = np.load('y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_full , y_train_full = train(forecast_features= True)\n",
    "# X_train_full.shape, y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(file= 'X_train_full', arr= X_train_full)\n",
    "# np.save(file= 'y_train_full', arr= y_train_full)\n",
    "\n",
    "X_full = np.load('X_train_full.npy')\n",
    "y_full = np.load('y_train_full.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Model Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) historical PV production data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X, y)\n",
    "model = compile_model(model, learning_rate= 1e-3)\n",
    "model, history = train_model(model, X, y)\n",
    "plot_loss_mae(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) historical PV production + historical weather-forecast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_full, y_full)\n",
    "model = compile_model(model, learning_rate=1e-3)\n",
    "model, history = train_model(model, X_full, y_full)\n",
    "plot_loss_mae(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerome-roeser/code/jerome-roeser/11-Personal-Projects/git_repos/advanced-power-forecast/power/ml_ops/data.py:268: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  years_df['hour_of_year'] = years_df.utc_time.\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utc_time</th>\n",
       "      <th>local_time</th>\n",
       "      <th>electricity</th>\n",
       "      <th>hour_of_year</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-31 00:00:00+00:00</td>\n",
       "      <td>2021-12-31 00:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-31 01:00:00+00:00</td>\n",
       "      <td>2021-12-31 01:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-31 02:00:00+00:00</td>\n",
       "      <td>2021-12-31 02:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-31 03:00:00+00:00</td>\n",
       "      <td>2021-12-31 03:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-31 04:00:00+00:00</td>\n",
       "      <td>2021-12-31 04:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>123104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2022-01-02 19:00:00+00:00</td>\n",
       "      <td>2022-01-02 19:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>010219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2022-01-02 20:00:00+00:00</td>\n",
       "      <td>2022-01-02 20:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>010220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2022-01-02 21:00:00+00:00</td>\n",
       "      <td>2022-01-02 21:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>010221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2022-01-02 22:00:00+00:00</td>\n",
       "      <td>2022-01-02 22:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>010222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2022-01-02 23:00:00+00:00</td>\n",
       "      <td>2022-01-02 23:00:00+00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>010223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    utc_time                 local_time  electricity  \\\n",
       "0  2021-12-31 00:00:00+00:00  2021-12-31 00:00:00+00:00          0.0   \n",
       "1  2021-12-31 01:00:00+00:00  2021-12-31 01:00:00+00:00          0.0   \n",
       "2  2021-12-31 02:00:00+00:00  2021-12-31 02:00:00+00:00          0.0   \n",
       "3  2021-12-31 03:00:00+00:00  2021-12-31 03:00:00+00:00          0.0   \n",
       "4  2021-12-31 04:00:00+00:00  2021-12-31 04:00:00+00:00          0.0   \n",
       "..                       ...                        ...          ...   \n",
       "67 2022-01-02 19:00:00+00:00  2022-01-02 19:00:00+00:00          0.0   \n",
       "68 2022-01-02 20:00:00+00:00  2022-01-02 20:00:00+00:00          0.0   \n",
       "69 2022-01-02 21:00:00+00:00  2022-01-02 21:00:00+00:00          0.0   \n",
       "70 2022-01-02 22:00:00+00:00  2022-01-02 22:00:00+00:00          0.0   \n",
       "71 2022-01-02 23:00:00+00:00  2022-01-02 23:00:00+00:00          0.0   \n",
       "\n",
       "   hour_of_year  mean  median  std  skew  min  max  count  \n",
       "0        123100   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "1        123101   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "2        123102   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "3        123103   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "4        123104   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "..          ...   ...     ...  ...   ...  ...  ...    ...  \n",
       "67       010219   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "68       010220   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "69       010221   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "70       010222   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "71       010223   0.0     0.0  0.0   0.0  0.0  0.0     40  \n",
       "\n",
       "[72 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_baseline_data(input_date: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Return a numpy array for 3-days (before and ahead of sell date) statistics\n",
    "    of PV power prodcution for a sell date.\n",
    "\n",
    "    Input:\n",
    "    the sell date: 'YYYY-MM-DD' e.g. '2022-07-06'\n",
    "\n",
    "    Output:\n",
    "    numpy array of shape (72,11) for 72 hours of data with 11 features\n",
    "    Features:\n",
    "        {0:'utc_time', 1:'local_time', 2:'electricity', 3:'hour_of_year',\n",
    "        4:'mean', 5:'median', 6:'std', 7:'skew', 8'min', 9'max', 10:'count'}\n",
    "    \"\"\"\n",
    "    # collect input for postprocess\n",
    "    data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed_pv = pd.read_csv(data_processed_pv_cache_path)\n",
    "\n",
    "    data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "    stats_df = get_stats_table(data_processed_pv, capacity=False)\n",
    "\n",
    "    # get plot_df\n",
    "    plot_df = postprocess(input_date, data_processed_pv, stats_df)\n",
    "\n",
    "    # Send as dict from backend to frontend; NaNs have to be replaced\n",
    "    plot_df = plot_df.fillna(0.0)\n",
    "    # plot_dict = plot_df.to_dict()\n",
    "\n",
    "    return plot_df\n",
    "\n",
    "baseline_df = get_baseline_data(input_date= '2022-01-01')\n",
    "baseline_array = baseline_df.to_numpy()\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_array[-24:,4] # mean of day-ahead for keras lamdbda layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_array[:24, 2] # day before sell for keras lambda layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### API call ==================================================================\n",
    "#base_url = \"http://127.0.0.1:8000\"\n",
    "base_url = \"https://power-v2-pdymu2v2na-ew.a.run.app\"\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def call_visu(today_date):\n",
    "\n",
    "    params_visu ={\n",
    "        'input_date': today_date,   # '2000-05-15' (dt.date())\n",
    "        'power_source': 'pv',\n",
    "        'capacity': 'true'\n",
    "        }\n",
    "\n",
    "    endpoint_visu = \"/visualisation\"\n",
    "    url_visu = f\"{base_url}{endpoint_visu}\"\n",
    "    response_visu = requests.get(url_visu, params_visu).json()\n",
    "\n",
    "    plot_df = pd.DataFrame.from_dict(response_visu)\n",
    "    plot_df.utc_time = pd.to_datetime(plot_df.utc_time,utc=True)\n",
    "\n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = call_visu('2022-12-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['min'].plot()\n",
    "plot_df['max'].plot()\n",
    "plot_df['mean'].plot()\n",
    "plot_df['cap_fac'].plot()\n",
    "plot_df['pred'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualisation(input_date: str, power_source='pv', capacity='false') -> None:\n",
    "  \"\"\"\n",
    "  input_date corresponds to \"today\"\n",
    "  \"\"\"\n",
    "\n",
    "  # collect input for postprocess\n",
    "  pred_df = pred( f\"{input_date} 12:00:00\")\n",
    "  data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "  preprocessed_df = pd.read_csv(data_processed_pv_cache_path)\n",
    "  preprocessed_df.utc_time = pd.to_datetime(preprocessed_df.utc_time,utc=True)\n",
    "\n",
    "  if capacity == 'true':\n",
    "    print('Capacity!')\n",
    "    preprocessed_df['cap_fac'] = preprocessed_df.electricity / 0.9 * 100 # 0.9 is max value for pv\n",
    "    stats_df = get_stats_table(preprocessed_df, capacity=True)\n",
    "    pred_df.pred = pred_df.pred / 0.9 * 100\n",
    "  else:\n",
    "    print('Electricity!')\n",
    "    stats_df = get_stats_table(preprocessed_df, capacity=False)\n",
    "\n",
    "  # get plot_df\n",
    "  plot_df = postprocess(input_date, preprocessed_df, stats_df, pred_df)\n",
    "\n",
    "  # Send as dict from backend to frontend; NaNs have to be replaced\n",
    "  plot_df = plot_df.fillna(0.0)\n",
    "  plot_dict = plot_df.to_dict()\n",
    "\n",
    "  return plot_dict\n",
    "\n",
    "plot_dict = visualisation('2022-07-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "\n",
    "# used in the plots\n",
    "today_date = st.session_state['today']\n",
    "plot_df = st.session_state['plot_df']\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "### capacity\n",
    "\n",
    "# time variables\n",
    "today_dt = pd.Timestamp(today_date, tz='UTC')\n",
    "time = plot_df.utc_time.values\n",
    "\n",
    "sep_future = today_dt + pd.Timedelta(days=1)\n",
    "sep_past = today_dt\n",
    "sep_order = today_dt + pd.Timedelta(hours=12)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ax.axvline(sep_past, color='k', linewidth=0.7)\n",
    "ax.axvline(sep_future, color='k', linewidth=0.7)\n",
    "ax.vlines(sep_order, ymin=0, ymax=100, color='k', linewidth=0.7, linestyle='--')\n",
    "\n",
    "# stats\n",
    "alpha_stats = 0.2\n",
    "ax.step(time, plot_df['min'].values, where='pre',\n",
    "        color='k', linestyle=':', alpha=alpha_stats, label='min')\n",
    "ax.step(time, plot_df['max'].values, where='pre',\n",
    "        color='k', linestyle=':', alpha=alpha_stats, label='max')\n",
    "ax.step(time, plot_df['mean'].values, where='pre',\n",
    "        color='k', linestyle='-', alpha=alpha_stats, label='mean')\n",
    "\n",
    "lower_bound = plot_df['mean'].values - 1 * plot_df['std'].values\n",
    "upper_bound = plot_df['mean'].values + 1 * plot_df['std'].values\n",
    "ax.fill_between(time, lower_bound, upper_bound, step='pre',\n",
    "                color='gray',\n",
    "                alpha=alpha_stats,\n",
    "                label='std')\n",
    "\n",
    "# true current production data\n",
    "current = 37 # current production data\n",
    "ax.step(time[:current], plot_df.cap_fac.values[:current], where='pre',\n",
    "        color='orange', linewidth=4, label='true')\n",
    "\n",
    "# prediction day ahead data\n",
    "hori = -24\n",
    "ax.step(time[hori:], plot_df.pred.values[hori:], where='pre',\n",
    "        color='orange', linewidth=4, linestyle=':', label='pred')\n",
    "\n",
    "###\n",
    "if show_true == 'Yes':\n",
    "    ax.step(time[-36:], plot_df.cap_fac.values[-36:], where='pre',\n",
    "         color='orange', linewidth=4, linestyle='-', alpha=0.4)\n",
    "    st.sidebar.write('')\n",
    "else:\n",
    "    st.sidebar.write('')\n",
    "\n",
    "# date ticks\n",
    "ax.xaxis.set_major_locator(dates.HourLocator(byhour=range(24), interval=12, tz='UTC'))\n",
    "ax.xaxis.set_major_formatter(dates.DateFormatter('%H:%M %d/%m/%Y'))\n",
    "\n",
    "ax.set_xlim(today_dt - pd.Timedelta(days=1), today_dt + pd.Timedelta(days=2))\n",
    "ax.set_ylim(0,120.0)\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Capacity factor in %')\n",
    "\n",
    "ax.annotate('Day-Ahead',(0.77,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Today',(0.48,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Day-Behind',(0.15,0.9), xycoords='subfigure fraction')\n",
    "ax.annotate('Order book closed',(0.51,0.77), xycoords='subfigure fraction')\n",
    "#ax.set_title(f\"Day Ahead prediction for { sep_future.strftime('%d/%m/%Y') }\")\n",
    "\n",
    "ax.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-Shvux39R-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
